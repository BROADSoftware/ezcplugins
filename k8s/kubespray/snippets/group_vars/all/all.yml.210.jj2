
# -------------------------------------- EZCLUSTER ADD ON

access_ip: "{{ ezcluster_ip }}" 
ip: "{{ ezcluster_ip }}"

# Override values from extra_playbooks/roles/container-engine/docker/defaults/main
docker_rh_repo_base_url: {{{m.data.repositories.docker_yum.docker_rh_repo_base_url}}}
docker_rh_repo_gpgkey: {{{m.data.repositories.docker_yum.docker_rh_repo_gpgkey}}}

dockerproject_rh_repo_base_url: {{{m.data.repositories.docker_yum.dockerproject_rh_repo_base_url}}}
dockerproject_rh_repo_gpgkey: {{{m.data.repositories.docker_yum.dockerproject_rh_repo_gpgkey}}}

{%% if m.data.httpProxies.docker is defined %%}    
# This will be used in docker proxy configuration  
{%% if m.data.httpProxies.docker.http_proxy is defined %%}    
docker_http_proxy: {{{m.data.httpProxies.docker.http_proxy}}}
{%% endif %%}
{%% if m.data.httpProxies.docker.https_proxy is defined %%}    
docker_https_proxy: {{{m.data.httpProxies.docker.https_proxy}}}
{%% endif %%}
{%% if m.data.httpProxies.docker.no_proxy is defined %%}    
docker_no_proxy: {{{m.data.httpProxies.docker.no_proxy}}}
{%% endif %%}
{%% endif %%}

{%% if m.data.httpProxies.yum is defined %%}    
# This will be used in yum proxy configuration  
{%% if m.data.httpProxies.yum.http_proxy is defined %%}    
yum_http_proxy: {{{m.data.httpProxies.yum.http_proxy}}}
{%% endif %%}
{%% if m.data.httpProxies.yum.https_proxy is defined %%}    
yum_https_proxy: {{{m.data.httpProxies.yum.https_proxy}}}
{%% endif %%}
{%% if m.data.httpProxies.yum.no_proxy is defined %%}    
yum_no_proxy: {{{m.data.httpProxies.yum.no_proxy}}}
{%% endif %%}
{%% endif %%}

{%% if m.data.repositories.kubespray_files is defined %%}
kubeadm_download_url: "{{{m.data.repositories.kubespray_files.kubeadm_download_url}}}"
{%% if m.data.repositories.kubespray_files.hyperkube_download_url is defined %%}
hyperkube_download_url: "{{{m.data.repositories.kubespray_files.hyperkube_download_url}}}"
{%% endif %%}
{%% if m.data.repositories.kubespray_files.kubectl_download_url is defined %%}
kubectl_download_url: "{{{m.data.repositories.kubespray_files.kubectl_download_url}}}"
{%% endif %%}
{%% if m.data.repositories.kubespray_files.kubelet_download_url is defined %%}
kubelet_download_url: "{{{m.data.repositories.kubespray_files.kubelet_download_url}}}"
{%% endif %%}
etcd_download_url: "{{{m.data.repositories.kubespray_files.etcd_download_url}}}"
cni_download_url: "{{{m.data.repositories.kubespray_files.cni_download_url}}}"
calicoctl_download_url: "{{{m.data.repositories.kubespray_files.calicoctl_download_url}}}"
crictl_download_url: "{{{m.data.repositories.kubespray_files.crictl_download_url}}}"
{%% endif %%}

# NB: To patch existing cluster, edit /etc/kubernetes/manifests/kube-apiserver.yaml on each master node
kube_kubeadm_apiserver_extra_args:
  default-not-ready-toleration-seconds: {{{m.cluster.k8s.kubespray.default_not_ready_toleration_seconds|default(300)}}}
  default-unreachable-toleration-seconds: {{{m.cluster.k8s.kubespray.default_unreachable_toleration_seconds|default(300) }}}

# ---------------- See kubespray/docs/kubernetes-reliability.md
# NB: To patch existing cluster, edit /etc/kubernetes/manifests/kube-controller-manager.yaml on each master node
# NB: This is now useless, as k8s now use taint based eviction, with default value defined below

# Default
#kubelet_status_update_frequency: 10s
#kube_controller_node_monitor_grace_period: 40s
#kube_controller_node_monitor_period: 5s
#kube_controller_pod_eviction_timeout: 5m0s

## Fast Update and Fast Reaction
# 
# In such scenario, pods will be evicted in **50s** because the node will be #considered as down after **20s**, and `--pod-eviction-timeout` occurs after
# **30s** more.  
# However, this scenario creates an overhead on etcd as every node will try to update its status every 2 seconds.
kubelet_status_update_frequency: 4s
kube_controller_node_monitor_grace_period: 20s
kube_controller_node_monitor_period: 2s
kube_controller_pod_eviction_timeout: 30s


## Medium Update and Average Reaction
#
# In that case, Kubelet will try to update status every 20s. So, it will be 6 * 5 = 30 attempts before Kubernetes controller manager will consider unhealthy status of node. 
# After 1m it will evict all pods. The total time will be 3m before eviction process.
# Such scenario is good for medium environments as 1000 nodes will require 3000 etcd updates per minute.
#kubelet_status_update_frequency: 20s
#kube_controller_node_monitor_grace_period: 2m
#kube_controller_node_monitor_period: 5s
#kube_controller_pod_eviction_timeout: 1m

# Low Update and Slow reaction
# In this scenario, every kubelet will try to update the status every minute. There will be 5 * 5 = 25 attempts before unhealthy status. After 5m,
# Kubernetes controller manager will set unhealthy status. This means that pods will be evicted after 1m after being marked unhealthy. (6m in total).
#kubelet_status_update_frequency: 1m
#kube_controller_node_monitor_grace_period: 5m
#kube_controller_node_monitor_period: 5s
#kube_controller_pod_eviction_timeout: 1m
